---
title: Giles' blog
date: "1970-01-01T00:00:00Z"
description: Giles' blog
params:
  feedlink: https://www.gilesthomas.com/feed/rss.xml
  feedtype: rss
  feedid: 050ad929ffbb32b9a175894a3d6a08d4
  websites:
    https://www.gilesthomas.com/: true
  blogrolls: []
  in_blogrolls:
  - title: RSS feeds from Minifeed.net
    description: ""
    id: 83b59248e9346428c889eb03522b4297
  recommended: []
  recommender: []
  categories: []
  relme: {}
  last_post_title: Why smart instruction-following makes prompt injection easier
  last_post_description: |-
    Back when I first started looking into LLMs,
    I noticed that I could use what I've since called the transcript hack
    to get LLMs to work as chatbots without specific fine-tuning.  It's occurred to
  last_post_date: "2025-11-12T19:00:00Z"
  last_post_link: https://www.gilesthomas.com/2025/11/smart-instruction-following-and-prompt-injection
  last_post_categories: []
  last_post_language: ""
  last_post_guid: 343d9351306bf8b4ab3874b0e2672a83
  score_criteria:
    cats: 0
    description: 3
    feedlangs: 0
    hasContent: 0
    hasPosts: 3
    postcats: 0
    promoted: 5
    promotes: 0
    relme: 0
    title: 3
    website: 2
  score: 16
  ispodcast: false
  isnoarchive: false
  innetwork: true
  language: ""
  postcount: 10
  avgpostlen: 0
---
