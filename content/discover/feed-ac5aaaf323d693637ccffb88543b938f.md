---
title: Eli Bendersky's website - Python
date: "2025-05-02T01:36:27-07:00"
description: ""
params:
  feedlink: https://eli.thegreenplace.net/feeds/python.atom.xml
  feedtype: atom
  feedid: ac5aaaf323d693637ccffb88543b938f
  websites: {}
  blogrolls: []
  in_blogrolls:
  - title: Planet Python
    description: ""
    id: 63826648a34be342fc027f97571f1a6c
  recommended: []
  recommender: []
  categories:
  - Machine Learning
  - Math
  - Python
  - misc
  relme: {}
  last_post_title: Notes on implementing Attention
  last_post_description: |-
    Some notes on implementing attention blocks in pure Python +
    Numpy. The focus here is on the exact implementation in code, explaining all the
    shapes throughout the process. The motivation for why
  last_post_date: "2025-05-02T01:36:27-07:00"
  last_post_link: https://eli.thegreenplace.net/2025/notes-on-implementing-attention/
  last_post_categories:
  - Machine Learning
  - Math
  - Python
  - misc
  last_post_language: ""
  last_post_guid: 033d301a77e945bdda0934e0b50cc7bf
  score_criteria:
    cats: 0
    description: 0
    feedlangs: 0
    hasContent: 3
    hasPosts: 3
    postcats: 3
    promoted: 5
    promotes: 0
    relme: 0
    title: 3
    website: 0
  score: 17
  ispodcast: false
  isnoarchive: false
  innetwork: true
  language: ""
  postcount: 10
  avgpostlen: 295
---
