---
title: Eli Bendersky's website - Python
date: "2025-04-18T16:33:37-07:00"
description: ""
params:
  feedlink: https://eli.thegreenplace.net/feeds/python.atom.xml
  feedtype: atom
  feedid: ac5aaaf323d693637ccffb88543b938f
  websites: {}
  blogrolls: []
  in_blogrolls:
  - title: Planet Python
    description: ""
    id: 63826648a34be342fc027f97571f1a6c
  recommended: []
  recommender: []
  categories:
  - Machine Learning
  - Math
  - Python
  - misc
  relme: {}
  last_post_title: Sparsely-gated Mixture Of Experts (MoE)
  last_post_description: |-
    In transformer models, the
    attention block
    is typically followed by a feed forward layer (FF), which is a simple fully-connected
    NN with a hidden layer and nonlinearity. Here's the code for such a
  last_post_date: "2025-04-18T16:33:37-07:00"
  last_post_link: https://eli.thegreenplace.net/2025/sparsely-gated-mixture-of-experts-moe/
  last_post_categories:
  - Machine Learning
  - Math
  - Python
  - misc
  last_post_language: ""
  last_post_guid: 9f4e51327fd04c58893b135b9a40f487
  score_criteria:
    cats: 0
    description: 0
    feedlangs: 0
    hasContent: 3
    hasPosts: 3
    postcats: 3
    promoted: 5
    promotes: 0
    relme: 0
    title: 3
    website: 0
  score: 17
  ispodcast: false
  isnoarchive: false
  innetwork: true
  language: ""
  postcount: 10
  avgpostlen: 295
---
